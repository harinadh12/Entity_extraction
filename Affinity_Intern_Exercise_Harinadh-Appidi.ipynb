{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Author : Harinadh Appidi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from spacy.tokens import DocBin\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_descriptor</th>\n",
       "      <th>store_number</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOLRTREE 2257 00022574 ROSWELL</td>\n",
       "      <td>2257</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUTOZONE #3547</td>\n",
       "      <td>3547</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TGI FRIDAYS 1485 0000</td>\n",
       "      <td>1485</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUFFALO WILD WINGS 003</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J. CREW #568 0</td>\n",
       "      <td>568</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           transaction_descriptor store_number dataset\n",
       "0  DOLRTREE 2257 00022574 ROSWELL         2257   train\n",
       "1                  AUTOZONE #3547         3547   train\n",
       "2           TGI FRIDAYS 1485 0000         1485   train\n",
       "3          BUFFALO WILD WINGS 003            3   train\n",
       "4                  J. CREW #568 0          568   train"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "importing data\n",
    "'''\n",
    "data = pd.read_csv('Summer Internship - Homework Exercise.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(x):\n",
    "    x = re.sub('\\W+',' ',x).split()\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "data.transaction_descriptor = data.transaction_descriptor.apply(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data to train, test and validation folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data.dataset=='train']\n",
    "val_data = data[data.dataset=='validation']\n",
    "test_data = data[data.dataset=='test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create custom entities to finetune spacy model learn new entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_data(data):\n",
    "    transaction_descriptor = data.transaction_descriptor.tolist()\n",
    "    store_number_list = data.store_number.tolist()\n",
    "    data =  [(descriptor, {'entities':[(re.search(store_number,descriptor).start(),re.search(store_number,descriptor).end(),'store_detector') ] }) for descriptor,store_number in zip(transaction_descriptor,store_number_list)]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dump newly created train, test, valid data to json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json','w',encoding='utf-8') as f:\n",
    "    json.dump(json_data(train_data),f)\n",
    "with open('valid.json','w',encoding='utf-8') as f:\n",
    "    json.dump(json_data(val_data),f)\n",
    "with open('test.json','w') as f:\n",
    "    json.dump(json_data(test_data),f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new blank spacy model and train, validation data in spacy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.blank('en') ## create blank spacy model\n",
    "# nlp = spacy.load('en_core_web_trf')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "\n",
    "def covert_spacy_format(TRAIN_DATA):\n",
    "    db = DocBin()\n",
    "    for text, annotations in tqdm(TRAIN_DATA):\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents=[]\n",
    "        for start, end, label in annotations['entities']:\n",
    "            span = doc.char_span(start, end, label=label,alignment_mode='expand')\n",
    "            if span is None:\n",
    "                print(\"skipping entity {} in {}\".format(label, text))\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    return (db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert train, validation data from json to spacy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2273.02it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2273.00it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('train.json','r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('valid.json','r') as f:\n",
    "    valid_data = json.load(f)\n",
    "    \n",
    "train_data = covert_spacy_format(train_data)\n",
    "train_data.to_disk('./data/train.spacy')\n",
    "valid_data = covert_spacy_format(valid_data)\n",
    "valid_data.to_disk('./data/valid.spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new config.cfg file from command line after copying default parameter values from spacy website to basic_config.cfg\n",
    "\n",
    "* python -m spacy init fill-config base_config.cfg config.cfg\n",
    "\n",
    "#### Train Spacy model from command line\n",
    "* python -m spacy train config.cfg --output ./large_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the spacy model and predict on test data\n",
    "1. For multiple predictions on a test sample, only first prediction is taken into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('large_out/model-best')\n",
    "docs = test_data.transaction_descriptor.tolist()\n",
    "store_numbers=[]\n",
    "for doc in docs:\n",
    "    doc= nlp(doc)\n",
    "    temp =[]\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='store_detector':\n",
    "            temp.append(ent.text.lstrip('0'))\n",
    "            break\n",
    "    else:\n",
    "        temp.append('Not_Predicted')\n",
    "    store_numbers.extend(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write test predictions to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['store_number_pred'] = store_numbers\n",
    "test_data.to_csv('test_data_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Accuracy score for Entity Extraction model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Accuracy ***** 0.9\n",
      "****** Precision ***** 0.9\n",
      "****** Recall ***** 0.9\n",
      "****** F1 Score ***** 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(\"***** Accuracy *****\",accuracy_score(test_data.store_number,test_data.store_number_pred))\n",
    "print(\"****** Precision *****\",precision_score(test_data.store_number,test_data.store_number_pred, average='micro'))\n",
    "print(\"****** Recall *****\",recall_score(test_data.store_number,test_data.store_number_pred,average='micro'))\n",
    "print(\"****** F1 Score *****\",f1_score(test_data.store_number,test_data.store_number_pred,average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps Followed to extract entities\n",
    "* I first observed the data to understand the problem of extracting entities from the transaction descriptors\n",
    "* some of the transaction descriptors are simple with only one number occuring where applying regex can find those\n",
    "* There are cases where applying regex or other rule based techniques may not be able to fetch right store number when there are other numbers exist along with store number.\n",
    "* My next approach was to build a learning based model to extract entities from these transaction descriptors.\n",
    "* With the input given to use pretrained model, I have gone with spacy model trained on large corpus of web text in english 'en_core_web_lg'.\n",
    "* This is because of my previous exposure to spacy models as they are trained using neural networks combining CNN , RNN, attention mechanisms and has been widely used in NLP community.\n",
    "* Spacy models are also well versed in named entity recognition tasks and comes with well defined documentation.\n",
    "\n",
    "##### Data Cleaning and Preparation:\n",
    "* Transaction descriptors are cleaned to have only numeric and alphabet characters and remove other characters.\n",
    "* data is split into train, valid,test data and converted to spacy format after adding new entity store_detector to custom train spacy model.\n",
    "#### Model Training:\n",
    "* Pretrained en_core_web_lg model is finetuned on train data and validated using validation data.\n",
    "#### Model Prediction and Metrics:\n",
    "* Best model is used for predictions on test data.\n",
    "* Accuracy score and other metircs are computed on the test data which is 90%.\n",
    " \n",
    "#### Other Notes:\n",
    "* I have also tried small language model and transformer based model (couldnt train completely on my laptop and had to abort before completion). However large model performed slightly better than small model with 1 point accuracy improvement.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1543c65faa7e3b41eb137e65d73820af7012ad87523169e4f9a6cb03cfd2f67"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
